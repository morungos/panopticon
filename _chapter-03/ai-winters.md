---
title: 'AI Winters'
order: 60
---

Artificial intelligence is one of the best examples of a technology that goes
through cycles, alternating between large investments and funding, followed
eventually by periods of disillusionment and an “AI Winter” where both funding
and people leave the field in droves.

To date, there have been two major AI Winters:

* **1973 to 1980** — substantially triggered by the “Lighthill Report”, a review into
  progress in the field, the report’s almost entirely negative appraisal of the
  more ambitious goals led to a very significant drop in research activity. However,
  this was also a point where the more interdisciplinary field of cognitive science
  started to diverge from AI.

* **1987 to 1993** — the end of the “expert systems” era, largely because the costs of
  building and maintaining large knowledge-rich systems had started to become
  prohibitive.

There are several possible explanations. The first is the most obvious — that
artificial intelligence was largely hype, and the winters were when people
started to see through it. There is likely some truth to this — researchers
were competing to justify their work to research funding agencies, and
sometimes they exaggerated to secure their funds.

However, think a more interesting explanation is that the winters correspond
with inflexion points where there is a mismatch between cost and value. 1973
was the beginning of the growth period of mainframe computing, and

Similarly, 1989 was the beginning of the growth period of powerful personal
workstations. Earlier specialized AI computers — the ubiquitous “Lisp
machines”, which costed up to $100,000 for a single-user workstation — began to
be replaced by more affordable UNIX workstations and even high-end PCs, so
again there was a new pressure and opportunity to move from “AI” technologies
to something more mainstream. So another way to see the “winters” is as the
lulls between periods of dramatic exploitation of hardware innovation.

The current growth in artificial intelligence is undoubtedly powered by the
application of hardware — initially GPUs, and now TPUs — as was to accelerate
and reduce the costs of building very large statistical models, which is pretty
much what most of modern artificial intelligence is based on.
