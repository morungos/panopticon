---
title: 'The problem of bias'
order: 70
---

Many of the reactions to the more spectacular failures in artificial
intelligence — good examples are the soap dispenser that failed for people of
colour, Twitter’s cropping algorithm, Amazon’s hiring systems — are that these
are simply a consequence of bias.

One of the foundational principles of positivism is that algorithms cannot be
biased, so any bias that does exist, must be a human factor, typically
introduced through the training set 
**I'm no longer convinced this is precise enough. Clarify and argue better**{: .todo}. 
There is, after all, an objective reality
out there, it's just a matter of finding it. This kind of selection bias, where
the data is not truly representative of the reality of the world, is a real
problem. Most data scientists and artificial intelligence developers are under
pressure to keep their work relatively affordable, so it is not always
economically viable to build a “perfect” data set, if it is even possible.

More likely, though, is that most of the time we exploit the information that we do have available
to us, and don’t truly think of the consequences — often because the
consequences themselves might be completely unknown.

Let’s pick one example: *confirmation bias*. This effect we tend to interpret
what we see in a way that confirms what we know and believe, rather than to
disprove it. Confirmation bias actually helps us see the wood for the trees.
It’s not perfect, and certainly means that we come to the wrong conclusions
from time to time, but it is a natural part of the human condition. Not a good
part, necessarily, but a natural part.

Confirmation bias is a challenging issue for data and artificial intelligence,
because it is exactly the kind of problem that allows the negative consequences
of models to leak through. By default, if we believe in GPT-3, we don’t look
for the problems, we look for evidence that it works, and resist the influence
of evidence that it doesn’t.

